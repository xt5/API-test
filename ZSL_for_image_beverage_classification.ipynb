{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pjwQxa7oC7X"
      },
      "source": [
        "# Tutorial: Zero-Shot Learning for Beverage Classification in Images\n",
        "\n",
        "In this tutorial we are going to implement Zero-Shot Learning (ZSL) using two popular deep learning models: CLIP (Contrastive Language-Image Pre-training) and LLaVA (Large Language and Vision Assistant model). This notebook is available in two forms:\n",
        "\n",
        "1.   [Online (Google Colab)](https://colab.research.google.com/github/ltu-capr/zsl-image-tutorial/blob/master/ZSL_for_image_beverage_classification.ipynb): For experimenting on Google's free platform without installing anything on your computer.\n",
        "2.   [Offline (Jupyter Notebook)](https://github.com/ltu-capr/zsl-image-tutorial): For experimenting locally on your own computer. This takes some additional setup, but is the best option for working with sensitive data.\n",
        "\n",
        "To run the code in a cell, either click inside it and press Ctrl + Enter, or click the 'play' button to the left of the cell.\n",
        "\n",
        "*The ZSL models at the core of this notebook run much faster with graphics processing unit (GPU) acceleration. If you are in Google Colab, you can enable GPU acceleration in the settings by going to Runtime > Change runtime type > Hardware accelerator (select \"GPU\").*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aVaTkcMgtu0"
      },
      "source": [
        "## Example scenario: Beverage Classification in Images\n",
        "\n",
        "In this scenario, we want to classify the type of beverage depicted in an image from a set of predefined categories, and thus, determine if an alcoholic beverage is present.\n",
        "\n",
        "The candidate beverage types we aim to classify are:\n",
        "\n",
        "*    Beer\n",
        "*    Wine\n",
        "*    Water\n",
        "*    Coffee\n",
        "*    Tea\n",
        "*    No Beverage\n",
        "\n",
        "To approach this, we will use two popular deep learning models that operate in different ways:\n",
        "\n",
        "* CLIP (Contrastive Language-Image Pre-training)\n",
        "* LLaVA (Large Language and Vision Assistant)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooNIV_sjpjeY"
      },
      "source": [
        "### Cell 0.0: Install software package requirements\n",
        "\n",
        "\n",
        "*   Pandas is used to load and save data in CSV (comma separated value) format.\n",
        "*   PyTorch and Transformers are used to run the models.\n",
        "*   bitsandbytes and accelerate are used to quantise the LLaVA model, such that we can run it on machines with smaller amounts of available memory.\n",
        "*   tqdm is used to show progress bars.\n",
        "*   scikit-learn is used for metric computation.\n",
        "*   matplotlib is used for visualising the confusion matrix and examples in the dataset.\n",
        "*   numpy is used for efficient numerical computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uafzJLsJsbqH",
        "collapsed": true,
        "outputId": "c4515eca-baba-4d18-a9cd-96aa74ddc0ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting transformers==4.46.0\n",
            "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.45.0\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting accelerate==1.2.0\n",
            "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.0) (0.5.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.0)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.45.0) (4.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.2.0) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.46.0) (2025.4.26)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\n",
            "Reason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.4/664.8 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pandas torch transformers==4.46.0 bitsandbytes==0.45.0 accelerate==1.2.0 tqdm scikit-learn matplotlib numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ugA61loYdj"
      },
      "source": [
        "### Cell 0.1: Import essential modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFnIfU23mRhY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import CLIPProcessor, CLIPModel, LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 0.2 Define useful functions\n",
        "Here we define some useful functions to help compute metrics we want to report for both models. The code here may look daunting, however it is not crucial to fully understand. Instead, you can run the code cell and move on to the next cell."
      ],
      "metadata": {
        "id": "mIKy7nbI4UK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def per_class_sensitivity_specificity(ground_truth_labels, predicted_labels, label_names):\n",
        "    \"\"\"Computes per-class sensitivity (recall) and specificity metrics.\n",
        "\n",
        "    Metrics returned in order of label_names.\n",
        "    \"\"\"\n",
        "    sensitivity_scores, specificity_scores = [], []\n",
        "    for label_name in label_names:\n",
        "        # Turn into binary problem. recall_score returns specificity then sensitivity\n",
        "        specificity, sensitivity = recall_score(\n",
        "            np.array(ground_truth_labels) == label_name, np.array(predicted_labels) == label_name,\n",
        "            pos_label=True, average=None, zero_division=np.nan)\n",
        "        sensitivity_scores.append(sensitivity)\n",
        "        specificity_scores.append(specificity)\n",
        "    return sensitivity_scores, specificity_scores\n",
        "\n",
        "\n",
        "def generate_metric_report(ground_truth_labels, predicted_labels, label_names):\n",
        "    \"\"\"Computes and displays a report containing a collection of metrics.\n",
        "\n",
        "    Per-class metrics include:\n",
        "    - Precision\n",
        "    - Recall (sensitivity)\n",
        "    - Specificity\n",
        "    - F1-Score (harmonic mean of precision and recall)\n",
        "\n",
        "    Overall metrics include:\n",
        "    - Accuracy\n",
        "    - Unweighted Average Recall (UAR)\n",
        "    \"\"\"\n",
        "    # Determine the length of the longest label name to format the table.\n",
        "    len_longest_label = max(len(label_name) for label_name in label_names)\n",
        "\n",
        "    # Compute per-class metrics.\n",
        "    per_class_precision = precision_score(ground_truth_labels, predicted_labels, labels=label_names, average=None, zero_division=np.nan)\n",
        "    per_class_recall, per_class_specificity = per_class_sensitivity_specificity(ground_truth_labels, predicted_labels, label_names=label_names)\n",
        "    per_class_f1_score = f1_score(ground_truth_labels, predicted_labels, labels=label_names, average=None, zero_division=np.nan)\n",
        "\n",
        "    # Display the per-class summary.\n",
        "    print(f'{\"\":^{len_longest_label}s}    {\"precision\":>9s}    {\"recall\":>6s}    {\"specificity\":>11s}    {\"f1-score\":>9s}    {\"support\":>6s}\\n')\n",
        "    for label_idx, label_name in enumerate(label_names):\n",
        "        print(f'{label_name:>{len_longest_label}s}    {per_class_precision[label_idx]:>9.2f}    {per_class_recall[label_idx]:>6.2f}    {per_class_specificity[label_idx]:>11.2f}    {per_class_f1_score[label_idx]:>9.2f}    {ground_truth_labels.count(label_name):>6d}')\n",
        "    print('\\n')\n",
        "\n",
        "    # Compute overall metrics.\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    uar = recall_score(ground_truth_labels, predicted_labels, labels=label_names, average='macro', zero_division=np.nan)\n",
        "\n",
        "    # Display the overall metrics.\n",
        "    print(f'{\"Number of Examples\":>35s}: {len(ground_truth_labels)}')\n",
        "    print(f'{\"Overall Accuracy\":>35s}: {accuracy*100:.2f}%')\n",
        "    print(f'{\"Unweighted Average Recall (UAR)\":>35s}: {uar:.2f}')"
      ],
      "metadata": {
        "id": "1ULw0KTS4Tsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiHKZs_KoYgE"
      },
      "source": [
        "### Cell 0.3: Load the \"beverage\" dataset\n",
        "\n",
        "To evaluate model performance across different beverage types and scenes, we have constructed a dataset consisting of the following beverages:\n",
        "\n",
        "**Alcoholic Beverages**\n",
        "*   Beer bottle\n",
        "*   Beer cup\n",
        "*   Red wine glass\n",
        "*   White wine glass\n",
        "\n",
        "\n",
        "**Non-Alcoholic Beverages**\n",
        "*   Coffee cup\n",
        "*   Coffee plunger\n",
        "*   Tea cup\n",
        "*   Water bottle\n",
        "*   Water cup\n",
        "\n",
        "\n",
        "Multiple images of each beverage have been captured across five distinct scenes, both indoor and outdoor, with the beverage positioned at three different distances from the camera: foreground, midground, and background. The image below shows examples of the images contained in the dataset:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/ltu-capr/zsl-image-tutorial/blob/main/dataset_example.jpg?raw=1\" width=\"70%\"/>\n",
        "</div>\n",
        "\n",
        "In total, for each of the nine beverage types, images have been taken across five different scenes and at three focal distances, resulting in 15 images per beverage. This leads to a complete dataset of 135 images (9 beverages $\\times$ 15 images each).\n",
        "\n",
        "**Note for offline notebook version**: You may encounter an error when downloading the image dataset when running the below code cell. If so:\n",
        "1. Manually download the dataset from [here](https://github.com/ltu-capr/zsl-image-tutorial/raw/refs/heads/main/Data/Images.zip).\n",
        "2. Extract the downloaded ZIP folder and place the folder in the same location as this notebook. The name of the folder should be 'Images', and inside the folder should be 135 images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the zipped image dataset.\n",
        "!wget -O images.zip 'https://github.com/ltu-capr/zsl-image-tutorial/raw/refs/heads/main/Data/Images.zip'\n",
        "!unzip -o -q images.zip -d ./Images\n",
        "\n",
        "# Load the CSV file containing the labels for each image.\n",
        "# Here we are giving the URL for a sample file that we've made publicly\n",
        "# available on the Internet.\n",
        "labels_location = 'https://raw.githubusercontent.com/ltu-capr/zsl-image-tutorial/main/Data/images_labelled.csv'\n",
        "beverage_dataframe = pd.read_csv(labels_location)"
      ],
      "metadata": {
        "id": "Mcp0m4-O0_jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vQlcHjJJnaj"
      },
      "source": [
        "### Cell 0.4: Visualise the \"beverage\" dataset\n",
        "Here we visualise a few of the examples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define how many images to show.\n",
        "rows, cols = 3, 6\n",
        "num_images = rows * cols\n",
        "\n",
        "# Create a 3x6 grid for displaying images.\n",
        "fix, axes = plt.subplots(3, 6, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for index, sample in beverage_dataframe[:num_images].iterrows():\n",
        "    # Load the image and create a small thumbnail.\n",
        "    im = Image.open('Images/' + sample['image_name'] + '.jpg')\n",
        "    im.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
        "\n",
        "    # Display the image, setting the title as the beverage type and focal location.\n",
        "    axes[index].imshow(im)\n",
        "    axes[index].set_title(f'{sample[\"beverage\"]}/{sample[\"position\"]}')\n",
        "\n",
        "    # Hide the axis markers.\n",
        "    axes[index].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "42agiLVw2N8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6S86q7qoYit"
      },
      "source": [
        "## Example 1: ZSL Classification of Beverages in Images Using CLIP\n",
        "\n",
        "CLIP works by taking an image and a collection of descriptors, and generates an image-text similarity score for each image-descriptor pair. With this, we can provide CLIP an image along with a set of candidate labels, and generate a prediction by using the candidate label with the highest image-text similarity score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk8x7uQFoYlK"
      },
      "source": [
        "### Cell 1.0: Initialise the CLIP model\n",
        "\n",
        "Initialise the CLIP model for use in zero-shot image classification. It may take a while for the model to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNvD9LjArfUx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Use the GPU if it's available.\n",
        "clip_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the pre-trained model.\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(clip_device)\n",
        "\n",
        "# Load an object used to prepare images in the right way required by CLIP\n",
        "# This will: resize, centrecrop, and normalise the images.\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BSyeK9doYns"
      },
      "source": [
        "### Cell 1.1: Initialise classification labels\n",
        "\n",
        "In order to perform classification, we must nominate candidate labels for the model to choose between. In this scenario we have 6x labels, however you can choose as many labels as you need.\n",
        "\n",
        "It is possible to further engineer the text in these labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX6QoZ2ruCSu"
      },
      "outputs": [],
      "source": [
        "candidate_labels = [\n",
        "    'A photo containing beer',\n",
        "    'A photo containing wine',\n",
        "    'A photo containing coffee',\n",
        "    'A photo containing tea',\n",
        "    'A photo containing water',\n",
        "    'A photo containing no drinks'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeOCo5CSuDth"
      },
      "source": [
        "### Cell 1.2: Make model predictions\n",
        "\n",
        "Here we generate predictions for all images in our dataset.\n",
        "\n",
        "In this example we also implement *batching*, enabling the model to process multiple images simultaneously. This is especially useful for larger datasets, as it can reduce computational overhead and speed up predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2FWBMbavl09"
      },
      "outputs": [],
      "source": [
        "# Set the batch size (how many images to process at once).\n",
        "clip_batch_size = 10\n",
        "\n",
        "# Generate prediction results for all images in our dataset.\n",
        "all_results = []\n",
        "for index in tqdm(range(0, len(beverage_dataframe), clip_batch_size)):\n",
        "    # Extract the next set of data to process.\n",
        "    next_data = beverage_dataframe.iloc[index:index + clip_batch_size]\n",
        "\n",
        "    # Open the images.\n",
        "    images = [Image.open('Images/' + sample['image_name'] + '.jpg') for _, sample in next_data.iterrows()]\n",
        "\n",
        "    # Preprocess the data to the correct format.\n",
        "    inputs = clip_processor(\n",
        "        text=candidate_labels,\n",
        "        images=images, return_tensors='pt', padding=True\n",
        "    ).to(clip_device)\n",
        "\n",
        "    # Get the model outputs.\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "\n",
        "    # Get the logits per-image (the image-text similarity score to each candidate label).\n",
        "    image_logits = outputs.logits_per_image.cpu()\n",
        "\n",
        "    # Convert logits per-image into probabilities with a softmax, then convert to a list.\n",
        "    image_probs = image_logits.softmax(dim=1)\n",
        "    image_probs = list(torch.unbind(image_probs, dim=0))\n",
        "\n",
        "    # Store the probabilities.\n",
        "    all_results.extend(image_probs)\n",
        "\n",
        "    # Visualise last result in batch as model is running.\n",
        "    last_image = images[-1]\n",
        "    last_image_name = next_data.iloc[-1]['image_name']\n",
        "    last_image_probs = all_results[-1]\n",
        "\n",
        "    # Determine the index of the highest probability.\n",
        "    highest_idx = torch.argmax(last_image_probs)\n",
        "\n",
        "    # Prepare the image for displaying\n",
        "    thumb_im = last_image.copy()\n",
        "    thumb_im.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
        "    display(thumb_im)\n",
        "\n",
        "    print(f'Most likely label for {last_image_name}: {candidate_labels[highest_idx]} (probability: {100 * last_image_probs[highest_idx].item():.1f}%)')\n",
        "    print(f'Per-class probabilities:')\n",
        "    for lbl_idx, label in enumerate(candidate_labels):\n",
        "        print(f'\\t{label}: {100 * last_image_probs[lbl_idx]:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgqkJrkwQS-K"
      },
      "source": [
        "### Cell 1.3 Save the model predictions\n",
        "\n",
        "This code prepares an output CSV file containing model predictions which can be used for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0NRDvJMQnzX"
      },
      "outputs": [],
      "source": [
        "def save_clip_predictions(file_name, dataframe, all_results, candidate_labels, actual_label_column=None):\n",
        "    # Arrange the results in tabular form with neat columns.\n",
        "    rows = []\n",
        "    for result, (index, sample) in zip(all_results, dataframe.iterrows()):\n",
        "        scores_as_percentages = [round(score * 100, 2) for score in result.tolist()]\n",
        "        row = {'image_name': sample['image_name'], **dict(zip(candidate_labels, scores_as_percentages))}\n",
        "        rows.append(row)\n",
        "    results_df = pd.DataFrame(rows, columns=['image_name', *candidate_labels])\n",
        "    results_df['predicted_label'] = results_df[candidate_labels].idxmax(axis=1)\n",
        "\n",
        "    if actual_label_column is not None and actual_label_column in dataframe.columns:\n",
        "        results_df['actual_label'] = 'A photo containing ' + dataframe[actual_label_column]\n",
        "\n",
        "    # Save output to a CSV file.\n",
        "    os.makedirs('Outputs', exist_ok=True)\n",
        "    output_file_name = os.path.join('Outputs', file_name)\n",
        "    results_df.to_csv(output_file_name, index=False)\n",
        "\n",
        "    try:\n",
        "        # If we are on Google Colab, download the results.\n",
        "        from google.colab import files\n",
        "        files.download(output_file_name)\n",
        "    except ModuleNotFoundError:\n",
        "        # If we are not on Google Colab, show the output file location.\n",
        "        print('Output file saved:')\n",
        "        print(os.path.abspath(output_file_name))\n",
        "\n",
        "\n",
        "save_clip_predictions('beverage_classification_clip_predictions.csv',\n",
        "                      beverage_dataframe, all_results, candidate_labels,\n",
        "                      'beverage_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EcxQEsOs9Qy"
      },
      "source": [
        "### Cell 1.4. Measure the accuracy of model predictions (optional)\n",
        "\n",
        "Zero-shot learning does not require hand-annotated labels to generate predictions, but they can be used to validate the model's accuracy. Here we compare the model's outputs with hand-annotated (ground truth) labels. If you don't have hand-annotated labels for your data, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 1.4a. Metric computation\n",
        "In the code cell below, we will generate a table that summarises the precision, recall, specificity and F1 score for each beverage type individually, in addition to other statistics summarising the overall model performance, such as the overall accuracy and unweighted average recall (UAR).\n",
        "\n",
        "In the generated report, *support* refers to the number of ground truth labels belonging to that class."
      ],
      "metadata": {
        "id": "gDATsLJdLKyK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBRRYtkdAaiB"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = [candidate_labels[torch.argmax(result)] for result in all_results]\n",
        "ground_truth_labels = list('A photo containing ' + beverage_dataframe['beverage_type'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=candidate_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL7WezTqVsze"
      },
      "source": [
        "These statistics show that CLIP only achieves an overall accuracy of 34.07%. Though there is no baseline for accuracy, this is still a poor result, and suggests further engineering of the classification labels may be needed, or a different model may be more appropriate for this data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 1.4b. Confusion matrix\n",
        "\n",
        "We can analyse the model's performance in a more detailed manner by visualising a confusion matrix. A confusion matrix shows how many dataset examples there are for each possible pair of true and predicted labels. Numbers which do not lie on the main diagonal of the matrix correspond to misclassifications. By inspecting the classification matrix, we can quickly observe specific classes that the model is performing poorly on. For example, in this case we can see the model is best at correctly identifying photos containing beer, wine, and coffee, however struggles with other beverage types, such as water."
      ],
      "metadata": {
        "id": "Fws7vAZpLNdo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZVbQpWLVs8W"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=candidate_labels, xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nePUCwnvWsip"
      },
      "source": [
        "#### Cell 1.4c. Foreground/midground/background performance\n",
        "\n",
        "We can take a closer look at what other factors might be causing this misclassification. For instance, we can look at the classification performance in foreground, midground, and background. In this case, we can see that the model is much better at making correct predictions when the beverage is in the foreground as opposed to the midground or background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ebePk2S4O7x"
      },
      "outputs": [],
      "source": [
        "for position in ('foreground', 'midground', 'background'):\n",
        "    predicted_labels = []\n",
        "    ground_truth_labels = []\n",
        "    for result, (index, sample) in zip(all_results, beverage_dataframe.iterrows()):\n",
        "        if sample['position'] == position:\n",
        "            predicted_labels.append(candidate_labels[torch.argmax(result)])\n",
        "            ground_truth_labels.append('A photo containing ' + sample['beverage_type'])\n",
        "\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    print(f'Accuracy for {position}: {accuracy:.2%}', flush=True)\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=candidate_labels, xticks_rotation='vertical', cmap='Blues', colorbar=False);\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 1.4d. Alcohol vs. not-alcohol evaluation\n",
        "\n",
        "Finally, we can take a look at how well the model performed at correctly classifying whether an image contains an alcoholic beverage (beer or wine). If no beverage was detected, we classify this as no alcohol present."
      ],
      "metadata": {
        "id": "0_nqKJfW8u6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = [candidate_labels[torch.argmax(result)] for result in all_results]\n",
        "predicted_labels = ['alcohol' if 'beer' in label or 'wine' in label else 'not alcohol' for label in predicted_labels]\n",
        "ground_truth_labels = list(beverage_dataframe['alcohol_notalcohol'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=['alcohol', 'not alcohol'])\n",
        "\n",
        "# Generate the confusion matrix.\n",
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=['alcohol', 'not alcohol'], xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ],
      "metadata": {
        "id": "VxLrD-Hl8ui0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoogcj1ss9TS"
      },
      "source": [
        "## Example 2 - ZSL Classification of Beverages in Images Using LLaVA\n",
        "\n",
        "LLaVA is a large multimodal model that can accept both images and text as input. With LLaVA, we can provide an image along with a question, such as \"What beverage is contained in the image?\" or \"Does the image contain an alcoholic beverage?\". To make the answer suitable for classification, we can also ask the model to restrict its response to a set of candidate categories or a simple yes or no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8V3lJfOuNj9"
      },
      "source": [
        "### Cell 2.0 Initialise the LLaVA model\n",
        "\n",
        "Initialise the LLaVA model for use in zero-shot image classification. It will take a while for the model to download.\n",
        "\n",
        "LLaVA is quite a large model, which by default might not be able to run on most machines due to memory constraints. To support most machines, we will use a quantised version of the model, that reduces the memory requirements at a small cost to accuracy.\n",
        "\n",
        "This process requires the use of a GPU, meaning that a GPU is required to run the quantised version of LLaVA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoRjpEhvh1LK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Must use the GPU when running a quantised LLaVA.\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError('Must have a GPU available to run a quantised version of LLaVA.')\n",
        "llava_device = 'cuda:0'\n",
        "\n",
        "# Specify which version of LLaVa we will use.\n",
        "model_id = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
        "\n",
        "# Set up parameters relating to model quantisation.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load the pre-trained model.\n",
        "llava_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    model_id, quantization_config=quantization_config)\n",
        "\n",
        "# Load an object used to prepare data in the right way required by LLaVA.\n",
        "llava_processor = LlavaNextProcessor.from_pretrained(\n",
        "    model_id, patch_size=llava_model.config.vision_config.patch_size,\n",
        "    vision_feature_select_strategy=llava_model.config.vision_feature_select_strategy,\n",
        ")\n",
        "llava_model.generation_config.pad_token_id = llava_processor.tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell 2.1 Initialise prompt\n",
        "\n",
        "In order to perform classification we must first create a prompt for the LLaVA model. Here, we will ask the model to identify the presence of any specific beverages (beer, wine, coffee, tea, water) or none in the image. This approach allows us to handle the task as as a multi-class classification problem, similar to what we did for CLIP.\n",
        "\n",
        "We also limit the maximum number of tokens (words or word parts) that LLaVA can generate in its response. Here, we set it to a low value (10) as we only expect a single-word answer, though this choice is less critical as our prompt specifically instructs LLaVA to respond with only one word. Raising this limit would let LLaVA generate longer responses, which can be useful if a more detailed answer is needed."
      ],
      "metadata": {
        "id": "63LAqulSB1qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we specify the prompt to give to LLaVA.\n",
        "prompt = 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, wine, coffee, tea, water, none. Answer in one word.'\n",
        "\n",
        "# Here we specify how many tokens (words or word parts) can be returned by LLaVa. We restrict this to a small number.\n",
        "max_new_tokens = 10"
      ],
      "metadata": {
        "id": "qN1YT0SPB-PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYyn05Kyk6Nv"
      },
      "source": [
        "### Cell 2.2 Make model predictions\n",
        "\n",
        "Here we generate predictions for all images in our dataset.\n",
        "\n",
        "In this example we also implement *batching*, enabling the model to process multiple images simultaneously. This is especially useful for larger datasets, as it can reduce computational overhead and speed up predictions.\n",
        "\n",
        "Depending on the GPU used, a smaller batch size may be required due to memory constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEGkhjDklkrF"
      },
      "outputs": [],
      "source": [
        "# Set the batch size (how many images to process at once).\n",
        "llava_batch_size = 10\n",
        "\n",
        "# Generate prediction results for all images in our dataset.\n",
        "all_results = []\n",
        "for index in tqdm(range(0, len(beverage_dataframe), llava_batch_size)):\n",
        "    # Extract the next set of data to process.\n",
        "    next_data = beverage_dataframe.iloc[index:index + llava_batch_size]\n",
        "\n",
        "    # Open the images.\n",
        "    images = [Image.open('Images/' + sample['image_name'] + '.jpg') for _, sample in next_data.iterrows()]\n",
        "\n",
        "    # Create the formatted prompt for LLaVA.\n",
        "    llava_prompt = f'[INST] <image>\\n{prompt} [/INST]'\n",
        "\n",
        "    # Prepare the inputs to pass to LLaVA.\n",
        "    inputs = llava_processor(\n",
        "        text=[llava_prompt] * len(images),\n",
        "        images=images, return_tensors='pt'\n",
        "    ).to(llava_device)\n",
        "\n",
        "    # Get the model predictions.\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Process all predictions.\n",
        "    for output in outputs:\n",
        "        # Get the generated text and extract just the response.\n",
        "        generated_text = llava_processor.decode(output.cpu(), skip_special_tokens=True)\n",
        "        response = generated_text.split(f'{prompt} [/INST]')[1].strip().lower()\n",
        "\n",
        "        # Store the response.\n",
        "        all_results.append(response)\n",
        "\n",
        "    # Visualise last result in batch as model is running.\n",
        "    last_image = images[-1]\n",
        "    last_image_name = next_data.iloc[-1]['image_name']\n",
        "    last_image_response = all_results[-1]\n",
        "\n",
        "    # Prepare the image for displaying\n",
        "    thumb_im = last_image.copy()\n",
        "    thumb_im.thumbnail((256, 256), Image.Resampling.LANCZOS)\n",
        "    display(thumb_im)\n",
        "\n",
        "    print(f'Image: {last_image_name}')\n",
        "    print(f'Prompt: {prompt}')\n",
        "    print(f'Response: {last_image_response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP8caeajppK2"
      },
      "source": [
        "### Cell 2.3 Save the model predictions\n",
        "\n",
        "This code prepares an output CSV file containing model predictions which can be used for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz8GMt95pzQ8"
      },
      "outputs": [],
      "source": [
        "def save_llava_predictions(file_name, dataframe, all_results, actual_label_column=None):\n",
        "    # Arrange the results in tabular form with neat columns.\n",
        "    results_df = pd.DataFrame([])\n",
        "    results_df['image_name'] = dataframe['image_name']\n",
        "    results_df['predicted_label'] = all_results\n",
        "\n",
        "    if actual_label_column is not None and actual_label_column in dataframe.columns:\n",
        "        results_df['actual_label'] = dataframe[actual_label_column]\n",
        "\n",
        "    # Save output to a CSV file.\n",
        "    os.makedirs('Outputs', exist_ok=True)\n",
        "    output_file_name = os.path.join('Outputs', file_name)\n",
        "    results_df.to_csv(output_file_name, index=False)\n",
        "\n",
        "    try:\n",
        "        # If we are on Google Colab, download the results.\n",
        "        from google.colab import files\n",
        "        files.download(output_file_name)\n",
        "    except ModuleNotFoundError:\n",
        "        # If we are not on Google Colab, show the output file location.\n",
        "        print('Output file saved:')\n",
        "        print(os.path.abspath(output_file_name))\n",
        "\n",
        "save_llava_predictions('beverage_classification_llava_predictions.csv',\n",
        "                       beverage_dataframe, all_results, 'beverage_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hJ_-1I06qWl"
      },
      "source": [
        "### Cell 2.4 Measure the accuracy of model predictions (optional)\n",
        "\n",
        "Zero-shot learning does not require hand-annotated labels to generate predictions, but they can be used to validate the model's accuracy. Here we compare the model's outputs with hand-annotated (ground truth) labels. If you don't have hand-annotated labels for your data, skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 2.4a. Metric computation\n",
        "\n",
        "In the code cell below, we will generate a table that summarises the precision, recall, specificity and F1 score for each beverage type individually, in addition to other statistics summarising the overall model performance, such as the overall accuracy and unweighted average recall (UAR).\n",
        "\n",
        "In the generated report, *support* refers to the number of ground truth labels belonging to that class."
      ],
      "metadata": {
        "id": "wXbehHc0LuzZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnesNHDH6qj3"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = all_results\n",
        "ground_truth_labels = list(beverage_dataframe['beverage_type'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=('beer', 'wine', 'coffee', 'tea', 'water', 'none'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N3b3MIJ7Crg"
      },
      "source": [
        "These results look much better than for CLIP, with an overall accuracy of 79.26%."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 2.4b. Confusion matrix\n",
        "\n",
        "We can analyse accuracy in a more detailed manner by visualising a confusion matrix. Here we can see the model does a really good job! In this case we can see the model often incorrectly classifies tea as coffee, however distinguishing between the two is very challenging!"
      ],
      "metadata": {
        "id": "R821iuiuLyOB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXhnDq687B9B"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=('beer', 'wine', 'coffee', 'tea', 'water', 'none'), xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHj_I2yupWzQ"
      },
      "source": [
        "#### Cell 2.4c. Foreground/midground/background performance\n",
        "\n",
        "We can further analyse the classification performance in foreground, midground, and background. Similarly to CLIP, the model performs better when the beverage is contained in the foreground, and worse when in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxb-gmEApVxt"
      },
      "outputs": [],
      "source": [
        "for position in ('foreground', 'midground', 'background'):\n",
        "    predicted_labels = []\n",
        "    ground_truth_labels = []\n",
        "    for result, (index, sample) in zip(all_results, beverage_dataframe.iterrows()):\n",
        "        if sample['position'] == position:\n",
        "            predicted_labels.append(result)\n",
        "            ground_truth_labels.append(sample['beverage_type'])\n",
        "\n",
        "    accuracy = accuracy_score(ground_truth_labels, predicted_labels)\n",
        "    print(f'Accuracy for {position}: {accuracy:.2%}', flush=True)\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=('beer', 'wine', 'coffee', 'tea', 'water', 'none'), xticks_rotation='vertical', cmap='Blues', colorbar=False);\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXfcOpohqZHs"
      },
      "source": [
        "#### Cell 2.4d. Alcohol vs. not-alcohol evaluation\n",
        "\n",
        "Finally, we can take a look at how well the model performed at correctly classifying whether an image contains an alcoholic beverage (beer or wine). If no beverage was detected, we classify this as no alcohol present. There were no instances where the model incorrectly labelled not alcohol as alcohol!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA0WfLmVqWEK"
      },
      "outputs": [],
      "source": [
        "# Put the predictions and ground truth into the right format.\n",
        "predicted_labels = ['alcohol' if label in {'beer', 'wine'} else 'not alcohol' for label in all_results]\n",
        "ground_truth_labels = list(beverage_dataframe['alcohol_notalcohol'])\n",
        "\n",
        "# Produce the metrics report.\n",
        "generate_metric_report(ground_truth_labels, predicted_labels, label_names=['alcohol', 'not alcohol'])\n",
        "\n",
        "# Generate the confusion matrix.\n",
        "ConfusionMatrixDisplay.from_predictions(ground_truth_labels, predicted_labels, labels=['alcohol', 'not alcohol'], xticks_rotation='vertical', cmap='Blues', colorbar=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGWQUTLw0-Ri"
      },
      "source": [
        "## Try Your Own Analysis\n",
        "\n",
        "When running the following cell, you will be asked to select/input your data file using widgets that appear directly below the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LmMqZjp1QY3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # If we are on Google Colab, show an upload widget.\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        file_locations = list(uploaded.keys())\n",
        "    else:\n",
        "        file_locations = ''\n",
        "except ModuleNotFoundError:\n",
        "    # If we are not on Google Colab, ask for the names of all files.\n",
        "    file_locations = []\n",
        "    while True:\n",
        "        next_location = input('Please enter the path to the image(s) you would like to classify. '\n",
        "                              'Type \\q to finish entering filepaths.')\n",
        "        if next_location == '\\q':\n",
        "            break\n",
        "\n",
        "for file_path in file_locations:\n",
        "    if not os.path.isfile(file_path):\n",
        "        print(f'File not found: {file_path}.')\n",
        "        print('Please run this cell again.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nOHTP7m1gBh"
      },
      "source": [
        "### CLIP\n",
        "\n",
        "*Ensure that you have run through all previous code cells in the \"Example 1\" section first, as this code makes use of the classifier we initialised in that part of the tutorial.*\n",
        "\n",
        "For simplicity, the code below does *not* use batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNeFQcqW2cKR"
      },
      "outputs": [],
      "source": [
        "# Here we specify the label options that the model will choose from.\n",
        "# Make sure that you update these options to suit your data and experiment\n",
        "# with different wordings.\n",
        "candidate_labels = [\n",
        "    'A photo containing beer',\n",
        "    'A photo containing wine',\n",
        "    'A photo containing water',\n",
        "    'A photo containing coffee',\n",
        "    'A photo containing tea',\n",
        "    'A photo containing no drinks',\n",
        "]\n",
        "\n",
        "# Put into a dataframe for compatibility with the previous code.\n",
        "custom_clip_dataframe = pd.DataFrame(file_locations, columns=['image_name'])\n",
        "\n",
        "# Generate prediction results.\n",
        "all_results = []\n",
        "for index, sample in tqdm(custom_clip_dataframe.iterrows(), total=len(custom_clip_dataframe)):\n",
        "    image = Image.open(sample['image_name'])\n",
        "    inputs = clip_processor(text=candidate_labels, images=image, return_tensors='pt', padding=True).to(clip_device)\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "    image_logits = outputs.logits_per_image[0].cpu()\n",
        "    image_probs = image_logits.softmax(dim=0)\n",
        "    all_results.append(image_probs)\n",
        "\n",
        "# Save output to a CSV file.\n",
        "save_clip_predictions('clip_predictions.csv', custom_clip_dataframe, all_results, candidate_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-TnDnEo1iWd"
      },
      "source": [
        "### LLaVA\n",
        "\n",
        "*Ensure that you have run through all previous code cells in the \"Example 2\" section first, as this code makes use of the classifier we initialised in that part of the tutorial.*\n",
        "\n",
        "For simplicity, the code below does *not* use batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0WdyLMs2tML"
      },
      "outputs": [],
      "source": [
        "# Here we specify the prompt for the model.\n",
        "# Make sure that you update the prompt to suit your data and experiment with\n",
        "# different wordings.\n",
        "prompt = 'What type of beverage does this image contain? Pick the most relevant one from this list: beer, wine, coffee, tea, water, none. Answer in one word.'\n",
        "\n",
        "# Here we specify how many tokens (words or word parts) can be returned by\n",
        "# LLaVa. Increase this limit to enable the model to give a more detailed response,\n",
        "# depending on the prompt.\n",
        "max_new_tokens = 10\n",
        "\n",
        "# Put into a dataframe for compatibility with the previous code.\n",
        "custom_llava_dataframe = pd.DataFrame(file_locations, columns=['image_name'])\n",
        "\n",
        "# Generate prediction results.\n",
        "all_results = []\n",
        "for index, sample in tqdm(custom_llava_dataframe.iterrows(), total=len(custom_llava_dataframe)):\n",
        "    image = Image.open(sample['image_name'])\n",
        "    llava_prompt = f'[INST] <image>\\n{prompt} [/INST]'\n",
        "    inputs = llava_processor(text=llava_prompt, images=image, return_tensors='pt').to(llava_device)\n",
        "    with torch.no_grad():\n",
        "        outputs = llava_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    generated_text = llava_processor.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
        "    response = generated_text.split(f'{prompt} [/INST]')[1].strip().lower()\n",
        "    all_results.append(response)\n",
        "\n",
        "# Save output to a CSV file.\n",
        "save_llava_predictions('llava_predictions.csv', custom_llava_dataframe, all_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mIKy7nbI4UK4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}